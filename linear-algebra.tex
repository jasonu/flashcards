\documentclass[avery5371,grid,letterpaper]{flashcards}

\cardfrontstyle[\large\slshape]{headings}
\cardbackstyle{empty}

\usepackage{amsfonts}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^{n}}
\newcommand{\Rm}{\mathbb{R}^{m}}
\newcommand{\nbyn}{n\!\times\!n}
\newcommand{\nbym}{n\times m}

\begin{document}

\cardfrontfoot{Linear Algebra}

\begin{flashcard}[Definition]{reduced row--echelon form}
\vspace*{\stretch{1}}
A matrix is in \textit{reduced row--echelon form} if all of the
following conditions are satisfied:
\begin{enumerate}
\item If a row has nonzero entries, then the first nonzero entry is 1.
\item If a column contains a leading 1, then all other entries in that
column are zero.
\item If a row contains a leading 1, then each row above contains a
leading 1 further to the left.
\end{enumerate}
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Definition]{rank}
\vspace*{\stretch{1}}
The \textit{rank} of a matrix $A$, is the number of leading 1s in $rref(A)$.
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Definition \& Theorem]{number of solutions of a linear system}
\vspace*{\stretch{1}}
If a system has at least one solution, then it is said to be \textit{consistent}.
If a system has no solutions, then it is said to be \textit{inconsistent}
(\textit{overdetermined}).

\bigskip
A \textit{consistent} system has either
\begin{itemize}
\item infinitely many solutions (\textit{underdeterminied})
\item exactly one solution (\textit{exactly determined})
\end{itemize}
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Definition]{linear combination}
\vspace*{\stretch{1}}
A \textit{linear combination} is a vector in $\Rn$ created by adding
together scalar multiples of other vectors in $\Rn$.  For example,
if $c_1, \ldots, c_m$ are in $\R$, then

\begin{displaymath}
\vec{x} = c_1\vec{v}_1 + c_2\vec{v}_2 + \cdots + c_m\vec{v}_m
\end{displaymath}

is a \textit{linear combination}.  We say $\vec{x}$ is
a \textit{linear combination} of $\vec{v}_1, \ldots, \vec{v}_m$.
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Definition]{subspaces of $\Rn$}
\vspace*{\stretch{1}}
A subset $W$ of $\Rn$ is called a \textit{subspace} of $\Rn$ if it has
the following three properties:
\begin{enumerate}
\item $W$ contains the zero vector for $\Rn$.
\item $W$ is closed under addition (if $\vec{w}_1$ and $\vec{w}_2$
are both in $W$, then so is $\vec{w}_1 + \vec{w}_2$).
\item $W$ is closed under scalar multiplication (if $\vec{w}$ is in $W$ and $k$ is any scalar, then $k\vec{w}$ is also in $W$).
\end{enumerate}
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{image and kernel are subspaces}
\vspace*{\stretch{1}}
If $T(\vec{x}) = A\vec{x}$ is a linear transformation from $\Rm$ to $\Rn$, then
\begin{itemize}
\item $ker(T)= ker(A)$ is a subspace of $\Rm$
\item $im(T) = im(A)$ is a subspace of $\Rn$
\end{itemize}
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Definition]{linear independence}
\vspace*{\stretch{1}}
Consider vectors $\vec{v}_1, \ldots, \vec{v}_m$ in $\Rn$.  These vectors are
said to be \textit{linearly independent} if none of them is a linear combination
of the preceding vectors.  One way to think of this is that none of the vectors
is redundant.

\medskip
Otherwise the vectors are said to be \textit{linearly dependent}.
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Definition]{basis}
\vspace*{\stretch{1}}
Consider vectors $\vec{v}_1, \ldots, \vec{v}_m$ from a subspace $V$ of
$\Rn$.  These vectors are said to form a \textit{basis} of $V$, if they
meet the following two requirements:
\begin{enumerate}
\item they span $V$
\item they are linearly independent
\end{enumerate}
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{number of vectors in a basis}
\vspace*{\stretch{1}}
All bases of a subspace $V$ of $\Rn$ consist of the same number of
vectors.  In other words, they all have the same dimension.
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Algorithm]{constructing a basis of the image}
\vspace*{\stretch{1}}
To construct a basis of the image of $A$, pick those column vectors of
$A$ that correspond to the columns of $rref(A)$ that contain leading 1s.
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Definition]{linear relations}
\vspace*{\stretch{1}}
Consider vectors $\vec{v}_1, \ldots, \vec{v}_m$ in $\Rn$.  An equation
of the form
\begin{displaymath}
c_1 \vec{v}_1, + \ldots + c_m \vec{v}_m = \vec{0}
\end{displaymath}
is called a \textit{linear relation} among the vectors
$\vec{v}_1, \ldots, \vec{v}_m$.  The \textit{trivial relation} with
$c_1, \ldots ,c_m = 0$ is always true.  \textit{Non-trivial relations}
(where at least one of the coefficients $c_i$ is nonzero) may or may not
exist among the vectors.
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{rank-nullity theorem}
\vspace*{\stretch{1}}
For any $\nbym$ matrix $A$ the following equation holds:
\begin{displaymath}
dim(im(A)) + dim(ker(A)) = m
\end{displaymath}
Alternatively, if we define the $dim(ker(A))$ to be the \textit{nullity}
of $A$, then we can rewrite the above as:
\begin{displaymath}
rank(A) + nullity(A) = m
\end{displaymath}
Some mathemeticians refer to this as the fundamental theorem of linear algebra.
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Definition]{dimension}
\vspace*{\stretch{1}}
For any subspace $V$ of $\Rn$, the number of vectors in a basis of $V$
is called the \textit{dimension} of $V$ and is denoted by $dim(V)$.
\vspace*{\stretch{1}}
\end{flashcard}



\begin{flashcard}[Definition]{linear transformation}
\vspace*{\stretch{1}}
A function $T$ that maps vectors from $\Rm$ to $\Rn$ is called a
\textit{linear transformation} if there is an $\nbym$ matrix $A$
such that
\begin{displaymath}
T(\vec{x}) = A\vec{x}
\end{displaymath}
for all $\vec{x}$ in $\Rm$.
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{linearity}
\vspace*{\stretch{1}}
A transformation $T$ is \textit{linear} iff (if and only if),
for all vectors $\vec{v}, \vec{w}$ and all scalars $k$
\begin{itemize}
\item $T(\vec{v} + \vec{w}) = T(\vec{v}) + T(\vec{w})$
\item $T(k \vec{v}) = k T(\vec{v})$
\end{itemize}
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{$det(A^{T}) = $}
\vspace*{\stretch{1}}
\begin{displaymath}
det(A^{T}) = det(A)
\end{displaymath}
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{$ker(A) = $}
\vspace*{\stretch{1}}
\begin{displaymath}
ker(A) = ker(A^{T}A)
\end{displaymath}
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{$(imA)^{\bot} = $}
\vspace*{\stretch{1}}
\begin{displaymath}
(imA)^{\bot} = ker(A^{T})
\end{displaymath}
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{$(AB)^{T} = $}
\vspace*{\stretch{1}}
\begin{displaymath}
(AB)^{T} = B^{T}A^{T}
\end{displaymath}
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Definition]{\textit{symmetric} and \\
\textit{skew-symmetric} matrices}
\vspace*{\stretch{1}}
Square matrix $A$ is \textit{symmetric} $\Leftrightarrow A^{T} = A$
\\
\\
Square matrix $A$ is \textit{skew-symmetric} $\Leftrightarrow A^{T} = -A$
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Definition]{orthogonal complement}
\vspace*{\stretch{1}}
Consider a subspace $V$ of $\Rn$.  The \textit{orthogonal complement}
$V^{\bot}$ of $V$ is the set of those vectors $\vec{x}$ in $\Rn$ that
are orthogonal to all vectors in $V$:
\begin{displaymath}
V^{\bot} = \lbrace \vec{x}: \vec{v}\cdot\vec{x} = 0, \forall \vec{v} \in V \rbrace
\end{displaymath}
Note that $V^{\bot}$ is the kernel of the orthogonal projection onto $V$.
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{properties of orthogonal matrices}
\vspace*{\stretch{1}}
\begin{small}
Consider an $n \times n$ matrix $A$.  The following statements are equivalent:
\begin{enumerate}
\item $A$ is an orthogonal matrix
\item The columns of $A$ form an orthonormal basis of $\Rn$
\item $A^{T}A = I_{n}$
\item $A^{-1} = A^{T}$
\item $\forall \vec{x} \in \mathbf{R}^{n} \quad \Vert A\vec{x} \Vert = \Vert \vec{x} \Vert \quad$ (preserves length)
\end{enumerate}
\end{small}
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{determinants of similar matrices}
\vspace*{\stretch{1}}
\begin{displaymath}
A \sim B \Rightarrow det(A) = det(B)
\end{displaymath}
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{determinant of an inverse}
\vspace*{\stretch{1}}
\begin{displaymath}
det(A^{-1}) = (det A)^{-1} = \frac{1}{det(A)}
\end{displaymath}
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{the determinant in terms of the columns}
\vspace*{\stretch{1}}
If A is an $n \times n$ matrix with columns, $\vec{v}_1, \ldots \vec{v}_n$, then,
\begin{displaymath}
\vert det(A) \vert =
\Vert \vec{v}_1 \Vert
\Vert {\vec{v}_2}^{\bot} \Vert
\cdots
\Vert {\vec{v}_n}^{\bot} \Vert
\end{displaymath}
where ${\vec{v}_1}^{\bot}, \ldots {\vec{v}_n}^{\bot}$ are defined as in the Gram-Schmidt process.
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{elementary row operations and determinants}
\vspace*{\stretch{1}}
For any $n \times n$ matrix $A$, and any scalar $k$:
\smallskip
\begin{small}
\begin{center}
\begin{tabular}{|p{3cm}|l|}
\hline
Elementary row op & Effect on determinant \\
\hline
scalar multiplication & $det(A) \rightarrow k \cdot det(A)$ \\
\hline
row swap & $det(A) \rightarrow -det(A)$ \\
\hline
multiple of one row added to another & $det(A) \rightarrow det(A)$ \\
\hline
\end{tabular}
\end{center}
\end{small}
\smallskip
Analogous results hold for column operations.
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Definition]{eigenvectors and eigenvalues}
\vspace*{\stretch{1}}
Consider an $\nbyn$ matrix $A$.  A \textbf{nonzero} vector $\vec{v}$ in $\Rn$ is called an \textit{eigenvector} of $A$ if $A\vec{v}$ is a scalar multiple of $\vec{v}$.  That is, if
\begin{displaymath}
A\vec{v} = \lambda\vec{v}
\end{displaymath}
for some scalar $\lambda$.  Note that this scalar may be zero.  This scalar $\lambda$ is caled the \textit{eigenvalue} associated with the eigenvector $\vec{v}$.
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{eigenvalues and characteristic equation}
\vspace*{\stretch{1}}
The eigenvalues of an $\nbyn$ matrix $A$ correspond to the solutions of the \textit{characteristic equation} given by:
\begin{displaymath}
\vert A - \lambda I \vert = 0
\end{displaymath}
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Definition]{trace}
\vspace*{\stretch{1}}
The sum of the diagonal entries of a square matrix $A$ is called the \textit{trace} of $A$, and is denoted by $tr(A)$.
\vspace*{\stretch{1}}
\end{flashcard}

\begin{flashcard}[Theorem]{characteristic equation of a $2\!\times\!2$ matrix}
\vspace*{\stretch{1}}
Given a $2\!\times\!2$ matrix $A$:
\begin{displaymath}
det(A - \lambda I) = \lambda^2 - tr(A)\lambda + det(A) = 0
\end{displaymath}
\vspace*{\stretch{1}}
\end{flashcard}


\end{document}
